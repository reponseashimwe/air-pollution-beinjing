{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Beijing Air Quality Forecasting - Enhanced Model\n",
        "\n",
        "## Project Overview\n",
        "Predict PM2.5 concentrations in Beijing using advanced LSTM networks for time series forecasting.\n",
        "\n",
        "**Objective**: Achieve RMSE < 3000 on Kaggle leaderboard  \n",
        "**Dataset**: 30,677 hourly observations (2010-2013) with weather features and PM2.5 concentrations  \n",
        "**Approach**: Advanced feature engineering, time series sequences, and systematic LSTM experimentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Configuration\n",
        "plt.style.use('default')\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Submission tracking ready!\n"
          ]
        }
      ],
      "source": [
        "# Setup submission tracking system\n",
        "def save_submission(predictions, experiment_name, test_index):\n",
        "    \"\"\"Save submission with timestamp and experiment info\"\"\"\n",
        "    os.makedirs('submissions', exist_ok=True)\n",
        "    \n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    \n",
        "    submission = pd.DataFrame({\n",
        "        'row ID': test_index.strftime('%Y-%m-%d %-H:%M:%S'),\n",
        "        'pm2.5': predictions.round().astype(int)\n",
        "    })\n",
        "    \n",
        "    filename = f'submissions/{timestamp}_{experiment_name}.csv'\n",
        "    submission.to_csv(filename, index=False)\n",
        "    \n",
        "    print(f\"✅ Submission saved: {filename}\")\n",
        "    print(f\"📊 Predictions - Min: {predictions.min():.1f}, Max: {predictions.max():.1f}\")\n",
        "    \n",
        "    return filename, submission\n",
        "\n",
        "print(\"🎯 Submission tracking ready!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Loading and Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Training data: (30676, 12)\n",
            "📊 Test data: (13148, 11)\n",
            "\n",
            "📋 Columns: ['No', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'datetime', 'cbwd_NW', 'cbwd_SE', 'cbwd_cv', 'pm2.5']\n",
            "\n",
            "❓ Missing values:\n",
            "Train: 1921\n",
            "Test: 0\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "train = pd.read_csv('./data/train.csv')\n",
        "test = pd.read_csv('./data/test.csv')\n",
        "\n",
        "print(f\"📊 Training data: {train.shape}\")\n",
        "print(f\"📊 Test data: {test.shape}\")\n",
        "print(f\"\\n📋 Columns: {list(train.columns)}\")\n",
        "\n",
        "# Missing values check\n",
        "print(f\"\\n❓ Missing values:\")\n",
        "print(f\"Train: {train.isnull().sum().sum()}\")\n",
        "print(f\"Test: {test.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📅 Time range - Train: 2010-01-01 00:00:00 to 2013-07-02 03:00:00\n",
            "📅 Time range - Test: 2013-07-02 04:00:00 to 2014-12-31 23:00:00\n",
            "\n",
            "🔍 PM2.5 statistics:\n",
            "Mean: 100.8, Std: 93.1\n",
            "Min: 0.0, Max: 994.0\n"
          ]
        }
      ],
      "source": [
        "# Convert datetime and set as index\n",
        "train['datetime'] = pd.to_datetime(train['datetime'])\n",
        "test['datetime'] = pd.to_datetime(test['datetime'])\n",
        "\n",
        "train.set_index('datetime', inplace=True)\n",
        "test.set_index('datetime', inplace=True)\n",
        "\n",
        "print(f\"📅 Time range - Train: {train.index.min()} to {train.index.max()}\")\n",
        "print(f\"📅 Time range - Test: {test.index.min()} to {test.index.max()}\")\n",
        "\n",
        "# Key insights\n",
        "if 'pm2.5' in train.columns:\n",
        "    print(f\"\\n🔍 PM2.5 statistics:\")\n",
        "    print(f\"Mean: {train['pm2.5'].mean():.1f}, Std: {train['pm2.5'].std():.1f}\")\n",
        "    print(f\"Min: {train['pm2.5'].min():.1f}, Max: {train['pm2.5'].max():.1f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Handling missing values...\n",
            "✅ After treatment - Train: 0, Test: 0\n",
            "📊 Clean datasets - Train: (30676, 11), Test: (13148, 10)\n"
          ]
        }
      ],
      "source": [
        "# Handle missing values\n",
        "print(\"🔧 Handling missing values...\")\n",
        "\n",
        "# For training data\n",
        "train_clean = train.copy()\n",
        "train_clean = train_clean.fillna(method='ffill')  # Forward fill\n",
        "train_clean = train_clean.fillna(method='bfill')  # Backward fill\n",
        "train_clean = train_clean.interpolate(method='linear')  # Linear interpolation\n",
        "\n",
        "# For test data\n",
        "test_clean = test.copy()\n",
        "test_clean = test_clean.fillna(method='ffill')\n",
        "test_clean = test_clean.fillna(method='bfill')\n",
        "test_clean = test_clean.interpolate(method='linear')\n",
        "\n",
        "print(f\"✅ After treatment - Train: {train_clean.isnull().sum().sum()}, Test: {test_clean.isnull().sum().sum()}\")\n",
        "print(f\"📊 Clean datasets - Train: {train_clean.shape}, Test: {test_clean.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Advanced Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🛠️ Feature engineering functions defined.\n"
          ]
        }
      ],
      "source": [
        "def create_advanced_features(df, target_col='pm2.5'):\n",
        "    \"\"\"Create lag features, rolling statistics, and temporal features\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "    \n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    \n",
        "    # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    \n",
        "    if target_col in df_enhanced.columns:\n",
        "        # Lag features for PM2.5\n",
        "        for lag in [1, 2, 3, 6, 12, 24]:\n",
        "            df_enhanced[f'pm2.5_lag_{lag}'] = df_enhanced[target_col].shift(lag)\n",
        "        \n",
        "        # Rolling statistics for PM2.5\n",
        "        for window in [6, 12, 24, 48]:\n",
        "            df_enhanced[f'pm2.5_roll_mean_{window}'] = df_enhanced[target_col].rolling(window).mean()\n",
        "            df_enhanced[f'pm2.5_roll_std_{window}'] = df_enhanced[target_col].rolling(window).std()\n",
        "    \n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    \n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "    \n",
        "    # Drop original temporal columns (keep encoded versions)\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "    \n",
        "    return df_enhanced\n",
        "\n",
        "def create_test_features(df):\n",
        "    \"\"\"Create features for test data (without PM2.5 lag features)\"\"\"\n",
        "    df_enhanced = df.copy()\n",
        "    \n",
        "    # Temporal features\n",
        "    df_enhanced['hour'] = df_enhanced.index.hour\n",
        "    df_enhanced['day_of_week'] = df_enhanced.index.dayofweek\n",
        "    df_enhanced['month'] = df_enhanced.index.month\n",
        "    df_enhanced['season'] = (df_enhanced.index.month % 12 + 3) // 3\n",
        "    \n",
        "    # Cyclical encoding for temporal features\n",
        "    df_enhanced['hour_sin'] = np.sin(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['hour_cos'] = np.cos(2 * np.pi * df_enhanced['hour'] / 24)\n",
        "    df_enhanced['day_sin'] = np.sin(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['day_cos'] = np.cos(2 * np.pi * df_enhanced['day_of_week'] / 7)\n",
        "    df_enhanced['month_sin'] = np.sin(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    df_enhanced['month_cos'] = np.cos(2 * np.pi * df_enhanced['month'] / 12)\n",
        "    \n",
        "    # Weather interaction features\n",
        "    df_enhanced['temp_dewp_diff'] = df_enhanced['TEMP'] - df_enhanced['DEWP']\n",
        "    df_enhanced['wind_pressure'] = df_enhanced['Iws'] * df_enhanced['PRES']\n",
        "    \n",
        "    # Wind direction features\n",
        "    wind_cols = [col for col in df_enhanced.columns if 'cbwd' in col]\n",
        "    if len(wind_cols) >= 2:\n",
        "        df_enhanced['wind_complexity'] = sum(df_enhanced[col] for col in wind_cols)\n",
        "    \n",
        "    # Drop original temporal columns\n",
        "    df_enhanced = df_enhanced.drop(['hour', 'day_of_week', 'month'], axis=1)\n",
        "    \n",
        "    return df_enhanced\n",
        "\n",
        "print(\"🛠️ Feature engineering functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Creating advanced features...\n",
            "📊 Original features: 11\n",
            "📊 Enhanced train features: 35\n",
            "📊 Enhanced test features: 20\n",
            "✨ New features added: 24\n",
            "🔧 Handling remaining NaN values...\n",
            "✅ Final datasets - Train: (30676, 35), Test: (13148, 20)\n"
          ]
        }
      ],
      "source": [
        "# Apply feature engineering\n",
        "print(\"🔧 Creating advanced features...\")\n",
        "train_enhanced = create_advanced_features(train_clean)\n",
        "test_enhanced = create_test_features(test_clean)\n",
        "\n",
        "print(f\"📊 Original features: {train_clean.shape[1]}\")\n",
        "print(f\"📊 Enhanced train features: {train_enhanced.shape[1]}\")\n",
        "print(f\"📊 Enhanced test features: {test_enhanced.shape[1]}\")\n",
        "print(f\"✨ New features added: {train_enhanced.shape[1] - train_clean.shape[1]}\")\n",
        "\n",
        "# Handle remaining NaNs (from lag and rolling features)\n",
        "print(\"🔧 Handling remaining NaN values...\")\n",
        "train_enhanced = train_enhanced.fillna(method='bfill').fillna(0)\n",
        "test_enhanced = test_enhanced.fillna(method='bfill').fillna(0)\n",
        "\n",
        "print(f\"✅ Final datasets - Train: {train_enhanced.shape}, Test: {test_enhanced.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Model Training and Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 Model architecture defined.\n"
          ]
        }
      ],
      "source": [
        "# Time series sequence creation\n",
        "def create_sequences(data, target, sequence_length=24):\n",
        "    \"\"\"Create sequences for LSTM input\"\"\"\n",
        "    X, y = [], []\n",
        "    \n",
        "    for i in range(sequence_length, len(data)):\n",
        "        X.append(data[i-sequence_length:i])\n",
        "        y.append(target[i])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Enhanced LSTM model\n",
        "def create_enhanced_model(input_shape):\n",
        "    \"\"\"Create enhanced LSTM model\"\"\"\n",
        "    model = Sequential([\n",
        "        LSTM(128, activation='tanh', return_sequences=True, input_shape=input_shape),\n",
        "        Dropout(0.3),\n",
        "        LSTM(64, activation='tanh'),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Training configuration\n",
        "SEQUENCE_LENGTH = 24\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=10, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "print(\"🤖 Model architecture defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Aligning features between train and test...\n",
            "📊 Training features: 33\n",
            "📊 Test features: 19\n",
            "🎯 Common features: 19\n",
            "✅ Feature alignment complete. Using 19 common features.\n"
          ]
        }
      ],
      "source": [
        "# Feature alignment for consistent training and testing\n",
        "print(\"🔧 Aligning features between train and test...\")\n",
        "\n",
        "# Get common features between train and test\n",
        "train_feature_cols = [col for col in train_enhanced.columns if col not in ['pm2.5', 'No']]\n",
        "test_feature_cols = [col for col in test_enhanced.columns if col != 'No']\n",
        "\n",
        "# Find common features\n",
        "common_features = [col for col in train_feature_cols if col in test_feature_cols]\n",
        "print(f\"📊 Training features: {len(train_feature_cols)}\")\n",
        "print(f\"📊 Test features: {len(test_feature_cols)}\")\n",
        "print(f\"🎯 Common features: {len(common_features)}\")\n",
        "\n",
        "# Use only common features\n",
        "X_train_common = train_enhanced[common_features]\n",
        "y_train = train_enhanced['pm2.5']\n",
        "X_test_common = test_enhanced[common_features]\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_common)\n",
        "X_test_scaled = scaler.transform(X_test_common)\n",
        "\n",
        "print(f\"✅ Feature alignment complete. Using {len(common_features)} common features.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏗️ Creating time series sequences...\n",
            "📊 Sequences shape: (30652, 24, 19)\n",
            "📊 Train: (24521, 24, 19), Validation: (6131, 24, 19)\n",
            "🚀 Training enhanced LSTM model...\n",
            "Epoch 1/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - loss: 8592.3135 - mae: 61.5735 - val_loss: 7658.8589 - val_mae: 56.6339 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - loss: 3818.4807 - mae: 40.4209 - val_loss: 6083.6274 - val_mae: 50.9194 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 2858.2891 - mae: 35.1973 - val_loss: 6053.7407 - val_mae: 50.9834 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 2283.5649 - mae: 31.7193 - val_loss: 5859.3711 - val_mae: 50.3037 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 1872.7279 - mae: 28.7286 - val_loss: 6234.5498 - val_mae: 51.8063 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - loss: 1562.7419 - mae: 26.3147 - val_loss: 6603.5347 - val_mae: 51.9961 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 1329.6766 - mae: 24.3643 - val_loss: 6317.5835 - val_mae: 51.2697 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 1193.0253 - mae: 23.0440 - val_loss: 6549.4424 - val_mae: 51.8182 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 1033.6483 - mae: 21.4597 - val_loss: 6833.0845 - val_mae: 52.6561 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 859.7446 - mae: 19.5156 - val_loss: 6500.6309 - val_mae: 51.7993 - learning_rate: 5.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 779.9894 - mae: 18.5628 - val_loss: 6645.5571 - val_mae: 52.1915 - learning_rate: 5.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 730.0067 - mae: 18.1170 - val_loss: 6512.7021 - val_mae: 51.5252 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 13ms/step - loss: 707.0151 - mae: 17.7696 - val_loss: 6665.6611 - val_mae: 51.8932 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - loss: 655.5553 - mae: 17.1397 - val_loss: 6753.5679 - val_mae: 52.2227 - learning_rate: 5.0000e-04\n",
            "\n",
            "🎯 Model Performance:\n",
            "Validation RMSE: 76.55\n",
            "Target: < 3000 RMSE\n",
            "🎉 TARGET ACHIEVED!\n"
          ]
        }
      ],
      "source": [
        "# Create sequences and train model\n",
        "print(\"🏗️ Creating time series sequences...\")\n",
        "X_seq, y_seq = create_sequences(X_train_scaled, y_train.values, SEQUENCE_LENGTH)\n",
        "\n",
        "# Train/validation split\n",
        "split_idx = int(0.8 * len(X_seq))\n",
        "X_train, X_val = X_seq[:split_idx], X_seq[split_idx:]\n",
        "y_train_seq, y_val = y_seq[:split_idx], y_seq[split_idx:]\n",
        "\n",
        "print(f\"📊 Sequences shape: {X_seq.shape}\")\n",
        "print(f\"📊 Train: {X_train.shape}, Validation: {X_val.shape}\")\n",
        "\n",
        "# Create and train model\n",
        "print(\"🚀 Training enhanced LSTM model...\")\n",
        "model = create_enhanced_model(X_train.shape[1:])\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train_seq,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "val_pred = model.predict(X_val, verbose=0)\n",
        "val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "\n",
        "print(f\"\\n🎯 Model Performance:\")\n",
        "print(f\"Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"Target: < 3000 RMSE\")\n",
        "\n",
        "if val_rmse < 3000:\n",
        "    print(\"🎉 TARGET ACHIEVED!\")\n",
        "else:\n",
        "    print(f\"📈 Need {val_rmse - 3000:.1f} points improvement\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔮 Generating test predictions...\n",
            "📊 Test sequences shape: (13148, 24, 19)\n",
            "🤖 Generating predictions...\n",
            "📊 Predictions - Min: 7.4, Max: 338.9\n",
            "✅ Submission saved: submissions/20250919_144052_enhanced_lstm_rmse_77.csv\n",
            "📊 Predictions - Min: 7.4, Max: 338.9\n",
            "\n",
            "🎯 EXPERIMENT COMPLETE!\n",
            "📈 Validation RMSE: 76.55\n",
            "💾 Submission saved: submissions/20250919_144052_enhanced_lstm_rmse_77.csv\n",
            "\n",
            "📋 Sample predictions:\n",
            "                row ID  pm2.5\n",
            "0   2013-07-02 4:00:00     33\n",
            "1   2013-07-02 5:00:00     33\n",
            "2   2013-07-02 6:00:00     33\n",
            "3   2013-07-02 7:00:00     32\n",
            "4   2013-07-02 8:00:00     31\n",
            "5   2013-07-02 9:00:00     30\n",
            "6  2013-07-02 10:00:00     28\n",
            "7  2013-07-02 11:00:00     26\n",
            "8  2013-07-02 12:00:00     25\n",
            "9  2013-07-02 13:00:00     24\n",
            "✅ Confirmed: submissions/20250919_144052_enhanced_lstm_rmse_77.csv exists in submissions folder!\n",
            "📁 File size: 301931 bytes\n"
          ]
        }
      ],
      "source": [
        "# Generate test predictions and create submission\n",
        "print(\"🔮 Generating test predictions...\")\n",
        "\n",
        "# Create sequences for test data - FIXED VERSION\n",
        "test_sequences = []\n",
        "last_train_sequence = X_train_scaled[-SEQUENCE_LENGTH:]\n",
        "\n",
        "for i in range(len(X_test_scaled)):\n",
        "    if i < SEQUENCE_LENGTH:\n",
        "        # For early predictions, use training data + available test data\n",
        "        needed_from_train = SEQUENCE_LENGTH - (i + 1)\n",
        "        if needed_from_train > 0:\n",
        "            sequence = np.vstack([X_train_scaled[-needed_from_train:], X_test_scaled[:i+1]])\n",
        "        else:\n",
        "            sequence = X_test_scaled[:SEQUENCE_LENGTH]\n",
        "    else:\n",
        "        # Use sliding window from test data only\n",
        "        sequence = X_test_scaled[i-SEQUENCE_LENGTH+1:i+1]\n",
        "    \n",
        "    # Ensure sequence is exactly SEQUENCE_LENGTH\n",
        "    if sequence.shape[0] != SEQUENCE_LENGTH:\n",
        "        if sequence.shape[0] < SEQUENCE_LENGTH:\n",
        "            padding_needed = SEQUENCE_LENGTH - sequence.shape[0]\n",
        "            padding = np.repeat(sequence[0:1], padding_needed, axis=0)\n",
        "            sequence = np.vstack([padding, sequence])\n",
        "        else:\n",
        "            sequence = sequence[-SEQUENCE_LENGTH:]\n",
        "    \n",
        "    test_sequences.append(sequence)\n",
        "\n",
        "# Convert to numpy array - all sequences should now have same shape\n",
        "X_test_seq = np.array(test_sequences)\n",
        "print(f\"📊 Test sequences shape: {X_test_seq.shape}\")\n",
        "\n",
        "# Generate predictions\n",
        "print(\"🤖 Generating predictions...\")\n",
        "test_predictions = model.predict(X_test_seq, verbose=0)\n",
        "test_predictions = np.maximum(test_predictions.flatten(), 0)  # Ensure non-negative\n",
        "\n",
        "print(f\"📊 Predictions - Min: {test_predictions.min():.1f}, Max: {test_predictions.max():.1f}\")\n",
        "\n",
        "# Create and save submission\n",
        "experiment_name = f\"enhanced_lstm_rmse_{val_rmse:.0f}\"\n",
        "filename, submission = save_submission(\n",
        "    test_predictions,\n",
        "    experiment_name,\n",
        "    test.index\n",
        ")\n",
        "\n",
        "print(f\"\\n🎯 EXPERIMENT COMPLETE!\")\n",
        "print(f\"📈 Validation RMSE: {val_rmse:.2f}\")\n",
        "print(f\"💾 Submission saved: {filename}\")\n",
        "print(f\"\\n📋 Sample predictions:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Check if submission file exists\n",
        "import os\n",
        "if os.path.exists(filename):\n",
        "    print(f\"✅ Confirmed: {filename} exists in submissions folder!\")\n",
        "    print(f\"📁 File size: {os.path.getsize(filename)} bytes\")\n",
        "else:\n",
        "    print(f\"❌ Warning: {filename} not found!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
